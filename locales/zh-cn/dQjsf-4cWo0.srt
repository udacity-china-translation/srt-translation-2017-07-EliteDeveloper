1
00:00:00,000 --> 00:00:04,000
先来回忆一下第二单位结尾学到的抓取网页代码

2
00:00:04,000 --> 00:00:09,000
我们用了两个变量 将 tocrawl 初始化为只包含种子页面的列表

3
00:00:09,000 --> 00:00:12,000
再用 tocrawl 来跟踪需要抓取的页面

4
00:00:12,000 --> 00:00:19,000
我们将 crawled 初始化为空列表  并用 crawled 来跟踪已被抓取的网页

5
00:00:19,000 --> 00:00:23,000
这样 只要仍有页面需要抓取 循环就会继续

6
00:00:23,000 --> 00:00:25,000
我们可弹出 (pop 操作) tocrawl 列表的最后一个网页

7
00:00:25,000 --> 00:00:32,000
如尚未抓取 则将该页面找到的所有链接合并放入到 tocrawl 中

8
00:00:32,000 --> 00:00:36,000
之后 再将该页面添加进抓取完毕的网页列表

9
00:00:36,000 --> 00:00:41,000
现在我们要了解 如何进行改变才可以不仅查找所有 URL

10
00:00:41,000 --> 00:00:42,000
还要构建我们自己的索引

11
00:00:42,000 --> 00:00:46,000
我们浏览页面的实际内容 并添加进索引

12
00:00:46,000 --> 00:00:52,000
因此 第一个变动就是更新索引 并改变返回结果

13
00:00:52,000 --> 00:00:56,000
所以我们最后不是返回 crawled 而是返回 index

14
00:00:56,000 --> 00:01:02,000
如希望对所有搜索过 URL 进行跟踪 则仍可返回 crawled 并在最后返回 crawled 和 index 两个结果

15
00:01:02,000 --> 00:01:05,000
但为了简单起见 这里仅仅返回 index

16
00:01:05,000 --> 00:01:09,000
要对搜索查询作出响应 这是切实需要的

17
00:01:09,000 --> 00:01:12,000
现在要做另一个很重要的变动

18
00:01:12,000 --> 00:01:19,000
我们要设法更新索引 来映射在抓取过的页面上找到的所有词

19
00:01:19,000 --> 00:01:22,000
这之前 我要先做个改动

20
00:01:22,000 --> 00:01:28,000
因为 get_all_links 和我们将词添加进索引的工作都依赖网页

21
00:01:28,000 --> 00:01:33,000
所以 要引入一个新变量 并将网页内容存储在于该变量

22
00:01:33,000 --> 00:01:37,000
这样就不用两次调用 get_page 了 这个过程其实挺昂贵的

23
00:01:37,000 --> 00:01:40,000
它需要一个网络请求来获得网页内容

24
00:01:40,000 --> 00:01:45,000
将之存储在新变量中更加合理 并可简化代码

25
00:01:45,000 --> 00:01:48,000
现在 我们只需传入 content

26
00:01:48,000 --> 00:01:51,000
这里缺少一个语句

27
00:01:51,000 --> 00:01:56,000
我希望大家能够找出这个语句 来完成网络爬虫

28
00:01:56,000 --> 00:01:59,000
完成后 crawl_web 的结果 即返回的 index

29
00:01:59,000 --> 00:02:02,000
应当就是从种子页面找到的所有内容的索引

